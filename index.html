<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards - Amirhossein Zamani, Tianhao Xie, Amir Aghdam, Tiberiu Popa, Eugene Belilovsky">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="3D texture generation, differentiable rewards, fine-tuning, RLHF, 3D computer vision, computer graphics, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Amirhossein Zamani, Tianhao Xie, Amir Aghdam, Tiberiu Popa, Eugene Belilovsky">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Mila - Quebec AI Institute and Concordia University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://ahhhz975.github.io/DifferentiableTextureLearning/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@Mila_Quebec">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AHHHZ975">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches.">
  <meta name="citation_author" content="Zamani, Amirhossein">
  <meta name="citation_author" content="Xie, Tianhao">
  <meta name="citation_author" content="Aghdam, Amir">
  <meta name="citation_author" content="Popa, Tiberiu">
  <meta name="citation_author" content="Belilovsky, Eugene">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2506.18331">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards",
    "description": "While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches.",
    "author": [
      {
        "@type": "Person",
        "name": "Amirhossein Zamani",
        "affiliation": {
          "@type": "Organization",
          "name": "Mila - Quebec AI Institute and Concordia University"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2025-12-05",
    "publisher": {
      "@type": "Organization",
      "name": "IEEE/CFV Winter Conference on Applications of Computer Vision (WACV) 2026"
    },
    "url": "https://ahhhz975.github.io/DifferentiableTextureLearning/",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://ahhhz975.github.io/DifferentiableTextureLearning/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">

    <section class="hero">
      <div style="top:0; left:0; right:0; display:flex; justify-content:space-between; align-items:center; padding:0.5rem 1rem; background:rgba(255,255,255,0.0);">
        <!-- LEFT logo (top-left corner) -->
        <a href="https://mila.quebec/en" style="display:inline-block; line-height:0;">
          <img src="static/images/Mila_Logo.png" alt="Mila logo" style="height:58px; max-height:58px; width:auto; display:block;">
        </a>
      
        <!-- RIGHT logos (two side-by-side) -->
        <div style="display:flex; gap:0.6rem; align-items:center;">
          <a href="https://www.concordia.ca/" style="display:inline-block; line-height:0;">
            <img src="static/images/Concordia_Logo.png" alt="Right logo 2" style="height:57px; max-height:58px; width:auto; display:block;">
          </a>
        </div>
      </div>
    </section>
    
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block"><a href="https://ahhhz975.github.io/" target="_blank">Amirhossein Zamani</a><sup>1,2*</sup>,</span>
              <span class="author-block"><a href="https://tianhaoxie.github.io/" target="_blank">Tianhao Xie</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://users.encs.concordia.ca/~aghdam/" target="_blank">Amir Aghdam</a><sup>2</sup>,</span>              
              <span class="author-block"><a href="https://users.encs.concordia.ca/~stpopa/" target="_blank">Tiberiu Popa</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://mila.quebec/en/directory/eugene-belilovsky" target="_blank">Eugene Belilovsky</a><sup>1,2</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->                    
                    <span class="author-block"><sup>1</sup> Mila – Quebec AI Institute; <sup>2</sup> Concordia University, Montreal, Canada <br><h1>IEEE/CFV Winter Conference on Applications of Computer Vision (WACV) 2026</h1></span>
                    <!-- TODO: Remove this line if no equal contribution -->
                     <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2506.18331" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/AHHHZ975/Differentiable-Texture-Learning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.18331" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <span class="link-block">
                  <a href="static/pdfs/WACV2026_Poster.pdf" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/CamPoseAware.jpg" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          1- Camera-Pose-Aware Reward: 
        </h2>
        <div class="content has-text-justified">          
          <p>
            Qualitative results of the camera-pose-aware texture generation experiment for different 
            text prompts on different 3D mesh objects. The objective of this experiment is to learn optimal camera viewpoints such that,
            when the object is rendered and textured from these views, the resulting texture maximizes the average aesthetic reward. 
            Consequently, by maximizing this reward, the model becomes invariant to the initial camera positions: regardless of where
            the cameras start, the training will adjust their azimuth and elevation to surround the 3D object in a way that yields high‑quality,
            aesthetically pleasing textures.
          </p>
        </div>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Symmetry.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          2- Symmetry-Aware Texture Generation Reward:
        </h2>
        <div class="content has-text-justified">          
          <p>
            Qualitative results of symmetry-aware texture generation experiment on a balloon mesh object.
            For each rwo (after and before fine-tuning), we show the rendered 3D object from multiple viewpoints, alongside the corresponding texture images
            (rightmost column), which highlight the symmetric regions. A vertical dashed line marks the symmetry axis in each texture image. 
            The purple plane passing through the center of the balloon in each viewpoint indicates the estimated symmetry plane of the object. 
            As shown, compared to the pre-trained model, our method generates textures that are more consistent across symmetric parts of the mesh. 
            Without symmetry supervision, patterns often differ noticeably between sides. In contrast, textures trained with the proposed symmetry reward 
            exhibit visually coherent features across symmetric regions, demonstrating the reward’s effectiveness in enforcing symmetry consistency.
          </p>
        </div>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MinTexGeoAlign.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         3- Geometry-Texture Alignment Reward:
       </h2>
       <div class="content has-text-justified">          
          <p>
            Qualitative results of the geometry-texture alignment experiment on a rabbit (bunny) mesh. For each row (before and after fine-tuning),
            we show the rendered 3D object from multiple viewpoints, with the corresponding texture image in the rightmost column. As shown, our method produces textures whose patterns 
            align more closely with the mesh’s curvature directions, unlike the pre-trained model. Moreover, a notable outcome in our results is the emergence of repetitive texture patterns
            after fine-tuning with the geometry-texture alignment reward. This behavior arises from the differentiable sampling strategy used during reward computation. Specifically, 
            it encourages the model to place edge features at specific UV coordinates which ultimately results in structured and repeated patterns in the texture.
          </p>
        </div>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/TexFeatEmph.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        4- Texture-Features Emphasis Reward:
      </h2>
      <div class="content has-text-justified">          
          <p>
            Qualitative results of the texture features emphasis experiment on a rabbit (bunny) object. For each row (before and after fine-tuning),
            we show the rendered 3D object from multiple viewpoints, with the corresponding texture image in the rightmost column. The goal of this experiment is to learn texture images 
            with salient features (e.g., edges) emphasized at regions of high surface bending, represented by the magnitude of mean curvature. This encourages texture patterns that highlight
            3D surface structure while preserving perceptual richness through color variation. As illustrated, our method enhances texture features, such as edges and mortar, in proportion to
            local curvature, a capability the pre-trained model lacks, often resulting in pattern-less (white) areas, particularly on the back and head of the rabbit.
          </p>
        </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- TODO: Replace with your YouTube video ID -->            
            <iframe width="560" height="315" src="https://www.youtube.com/embed/RNRSYCZ05uM?si=HlJimkTS1g0GTamC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
                While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper methodology -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Methodology</h2>
    </div>
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <img src="static/images/TrainingOverview.png" alt="Training overview" class="blend-img-background center-image">
    </div>
    <div class="content has-text-justified">          
      <p>
          An overview of the proposed training process, consisting of two main stages: (i) texture generation, where a latent diffusion model generates high-quality images from textual prompts. Combined with differentiable rendering and 3D vision techniques, this step produces realistic textures for 3D objects. (ii) texture reward learning, where an end-to-end differentiable pipeline fine-tunes the pre-trained text-to-image diffusion model by maximizing a differentiable reward function r. Gradients are back-propagated through the entire 3D generative pipeline, making the process inherently geometry-aware. To demonstrate the method’s effectiveness in producing textures aligned with 3D geometry, we introduce five novel geometry-aware reward functions.
      </p>
    </div>
  </div>
</section>
<!-- End paper methodology -->


<!-- Comparative Results-->
<section class="section hero is-light">
  <div class="container is-max-desktop">
     <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Comparative Results</h2>
     </div>
    <h2 class="subtitle has-text-centered"> Qualitative Comparison </h2>
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <img src="static/images/ComparativeResults.jpg" alt="Comparative results" class="blend-img-background center-image">
    </div>
    <h2 class="subtitle has-text-centered"> Quantitative Comparison </h2>
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <img src="static/images/QuantitativeResults.jpg" alt="Comparative results" class="blend-img-background center-image">
    </div>
  </div>
</section>
    

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">More Results</h2>
     </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/turntable4.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/turntable1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/turntable3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video4" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/turntable5.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video5">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video5" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/turntable2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video6">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video6" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/turntable6.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{zamani2025geometry,    
  title={Geometry-Aware Preference Learning for 3D Texture Generation},
  author={Zamani, AmirHossein and Xie, Tianhao and Aghdam, Amir G and Popa, Tiberiu and Belilovsky, Eugene},
  journal={arXiv preprint arXiv:2506.18331},
  year={2025},
  url={https://ahhhz975.github.io/DifferentiableTextureLearning/}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
